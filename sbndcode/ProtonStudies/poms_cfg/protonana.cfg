# POMS/fife_launch cfg single-stage template for analysis jobs
# for more details check https://cdcvs.fnal.gov/redmine/projects/fife_utils/wiki/Fife_launch_Reference
# make sure to replace all @TEMPLATE_...@ fields with meaningful values
[global]
# these are global parameters that can be used in any section
# and eventually overridden in stage sections
# users can add more parameters as needed to be used through the configuration
experiment = sbnd
account = mdeltutt
custom_tag = @TEMPLATE_TAG@
project_name = mdeltutt_protonana_bnbcosmic
release = v09_91_02_02
group = sbnd
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
larsoft_tag = v09_91_02_01
larsoft_qual = e26:prof
b_name = %(project_name)s
basename = override_me
fcl = override_me
startevent = 1
numevents = override_me
outdir = override_me
stage_name = override_me
artRoot_dataset = %(account)s_%(project_name)s_%(stage_name)s_%(custom_tag)s
histRoot_dataset = %(account)s_hist_%(project_name)s_%(stage_name)s_%(custom_tag)s


[env_pass]
# environment variable to pass to jobs
IFDH_DEBUG = 1
SAM_EXPERIMENT = %(experiment)s
# these are used by Project-py to identify output dataset
ARTROOT_DATASET = %(artRoot_dataset)s
HISTROOT_DATASET = %(histRoot_dataset)s
# XRootD env var to make it more resilient
XRD_CONNECTIONRETRY = 32
XRD_REQUESTTIMEOUT  = 14400
XRD_REDIRECTLIMIT   = 255
XRD_LOADBALANCERTTL = 7200
XRD_STREAMTIMEOUT   = 7200

# [prelaunch]
# script = source /cvmfs/larsoft.opensciencegrid.org/products/setups; source /cvmfs/fermilab.opensciencegrid.org/products/common/etc/setups; setup project_py; ups active; Project.py --prestage --defname %(sam_dataset)s --experiment sbnd 

[submit]
# jobsub_submit options
G = %(group)s
e = SAM_EXPERIMENT
e_1 = IFDH_DEBUG
e_2 = POMS4_CAMPAIGN_NAME
e_3 = POMS4_CAMPAIGN_STAGE_NAME
generate-email-summary = True
email-to = %(account)s@fnal.gov
# default resource requirements, these can be overridden in stage sections
expected-lifetime = 8h
disk = 10GB
memory = 2000MB
line = '+SingularityImage=\"/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest\"'
append_condor_requirements = '(TARGET.HAS_SINGULARITY=?=true)'


[job_setup]
# options related to user code setup
debug = True
find_setups = True
setup_local = True
source_1 = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1 = %(experiment)scode %(larsoft_tag)s -q %(larsoft_qual)s


[sam_consumer]
# SAM related options
limit = 1
schema = root
appname = %(experiment)scode
appvers = %(larsoft_tag)s
appfamily = art


[job_output]
# options to handle ArtRoot output files
addoutput = %(basename)s*.root
add_to_dataset = %(artRoot_dataset)s
dataset_exclude = hist*
rename = unique
dest = %(outdir)s
hash = 2
declare_metadata = True
metadata_extractor = sam_metadata_dumper
add_location = True
filter_metadata = data_stream
filter_metadata_2 = art.run_type
filter_metadata_3 = art.file_format_era
filter_metadata_4 = art.last_event
filter_metadata_5 = art.process_name
filter_metadata_6 = art.first_event
filter_metadata_7 = art.file_format_version


[job_output_1]
# options to handle ROOT output files
addoutput = hist_%(basename)s*.root
add_to_dataset = %(histRoot_dataset)s
rename = unique
dest = %(outdir)s
hash = 2
declare_metadata = True
add_location = True


[executable]
# executable options/arguments
# most of them are overridden in stage sections
name = true
arg_1 = -c
arg_2 = %(fcl)s
arg_3 = -o
arg_4 = %(basename)s_\${CLUSTER}_\${PROCESS}.root
arg_5 = -n
arg_6 = %(numevents)s
arg_7 = -T
arg_8 = hist_%(basename)s_\${CLUSTER}_\${PROCESS}.root


# In stage sections we can override parameter from each section using the form:
# section.key = value
# most options are using placeholder values
# resources need to be assigned realistic values

[stage_ana]
global.stage_name = ana
global.numevents = 100
global.fcl = run_protonanalyzer.fcl
global.basename = %(b_name)s_%(larsoft_tag)s_%(stage_name)s
global.outdir = /pnfs/%(experiment)s/scratch/users/%(account)s/%(release)s/%(stage_name)s/%(project_name)s
sam_consumer.limit = 1
executable.name = lar
job_setup.ifdh_art = True
submit.tar_file_name = dropbox:///path/to/tarball
submit.n_files_per_job = 1
submit.dataset = %(account)s_%(project_name)s_%(input_stage_name)s_%(custom_tag)s
submit.disk = 1GB
submit.expected-lifetime = 1h
submit.memory = 400MB
